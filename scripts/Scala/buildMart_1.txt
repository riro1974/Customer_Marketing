def buildMart(sc):
from pyspark.sql import HiveContext
from pyspark.sql import functions as F


sc.setLogLevel("ERROR")

hc= HiveContext(sc)
 
pos_df=  hc.sql("select * from CustMkt_Ext.pos")
customer_df =  hc.sql("select * from CustMkt_Ext.customer")
salesrep_df =  hc.sql("select * from CustMkt_Ext.salesrep")
dept_df=  hc.sql("select * from CustMkt_Ext.dept")
class_df=  hc.sql("select * from CustMkt_Ext.class")
vendor_df=  hc.sql("select * from CustMkt_Ext.vendor")
style_df=  hc.sql("select * from CustMkt_Ext.style")
store_df=  hc.sql("select * from CustMkt_Ext.store")
return_reason_df = hc.sql("select * from CustMkt_Ext.return_reason")
promotion_df = hc.sql("select * from CustMkt_Ext.promotion")

pos_df.registerTempTable("pos")
dept_df.registerTempTable("dept")
class_df.registerTempTable("class")
vendor_df.registerTempTable("vendor")
style_df.registerTempTable("style")
store_df.registerTempTable("store")
customer_df.registerTempTable("customer")
salesrep_df.registerTempTable("salesrep")
return_reason_df.registerTempTable("reason_cd")
promotion_df.registerTempTable("promotion")

# create product_id from MD5 function

from pyspark.sql.functions import concat,md5,decode,lit,current_date

pos_product_df = pos_df.withColumn("product_id",md5(concat(pos_df.dept_id,pos_df.class_id,pos_df.vendor_id,pos_df.style_id,pos_df.color,pos_df.size_id)))

pos_fact = pos_product_df.select("cust_id","order_id","promoind","promo_id","salesrep_id","store_id","product_id", "reg_unit_price","mkd_unit_price","sales_units","sales_dolrs","inv_units","reason_cd","transdate")

pos_sku = pos_product_df.select("product_id","dept_id","class_id","vendor_id","style_id","color","size_id");

product_df= pos_sku.distinct()

product_df_1 = product_df.withColumn("Dept_desc",lit('')).withColumn("Class_desc",lit('')).withColumn("Vendor_desc",lit('')).withColumn("Style_desc",lit('')).withColumn("Rating",lit(0)).withColumn("start_dt",current_date()).withColumn("End_dt",lit('2100-01-01'))

product_dim_df = product_df_1.select("product_id","dept_id","dept_desc","class_id","class_desc","vendor_id","vendor_desc","style_id","style_desc","color","size_id","rating","start_dt","End_dt")


# create fact tables

sales_fact_df = pos_fact.select("cust_id","order_id","promoind","promo_id","salesrep_id","store_id","product_id","reg_unit_price","mkd_unit_price","sales_units","sales_dolrs","transdate")

inv_fact_df = pos_fact.select("cust_id","order_id","promoind","promo_id","salesrep_id","store_id","product_id","reg_unit_price","mkd_unit_price","inv_units",F.when(pos_product_df.promoind == 0,pos_product_df.mkd_unit_price*pos_product_df.inv_units).otherwise(pos_product_df.reg_unit_price*pos_product_df.inv_units).alias('Inv_dolrs'),"transdate")

promo_fact_df = pos_fact.filter(pos_product_df.promoind == 0)

return_fact_df =  pos_fact.filter(pos_product_df.reason_cd != 0)

# create target hive tables

product_dim_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.product_dim")
sales_fact_df.write.format("orc").mode("overwrite").partitionBy("transdate").saveAsTable("custMkt_tgt.sales_fact")
inv_fact_df.write.format("orc").mode("overwrite").partitionBy("transdate").saveAsTable("custMkt_tgt.inv_fact")
promo_fact_df.write.format("orc").mode("overwrite").partitionBy("transdate").saveAsTable("custMkt_tgt.promotion_fact")
return_fact_df.write.format("orc").mode("overwrite").partitionBy("transdate").saveAsTable("custMkt_tgt.return_fact")
store_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.store_dim")
customer_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.customer_dim")
salesrep_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.salesep_dim")
return_reason_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.return_reason_dim")
promotion_df.write.format("orc").mode("overwrite").saveAsTable("custMkt_tgt.promotion_dim")

def main():
from pyspark import SparkContext
sc = SparkContext("local","BuildMart")
buildMart(sc)

main()

